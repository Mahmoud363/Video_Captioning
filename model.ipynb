{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt \n",
    "import cv2\n",
    "import pytube\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename='videodatainfo_2017.json'\n",
    "opened= open(filename, 'r')\n",
    "trainData=json.load(opened)\n",
    "opened.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos=trainData[\"videos\"]\n",
    "captions=trainData[\"sentences\"]\n",
    "newCap={}\n",
    "for sent in captions:\n",
    "    newCap[sent[\"video_id\"]]=[]\n",
    "for sent in captions:\n",
    "    newCap[sent[\"video_id\"]].append(sent[\"caption\"])\n",
    "captions=newCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a female voice over discusses japan\n"
     ]
    }
   ],
   "source": [
    "#vCaptions={}\n",
    "vCaptions=[] #list of valid captions\n",
    "vFeatures=[] #list of valid video features npy files \n",
    "for root, dirs, files in os.walk(\"features\"):\n",
    "    for filename in files:\n",
    "        filename=filename.split(\".\")\n",
    "        captions_size = len(captions[filename[0]])\n",
    "        \n",
    "        vCaptions.extend(captions[filename[0]])\n",
    "        vFeatures.extend(['features/'+filename[0]]*captions_size)\n",
    "\n",
    "\n",
    "print(vCaptions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tokens(val):\n",
    "    return f\"<start> {val} <end>\"\n",
    "\n",
    "vCaptions = map(add_tokens, vCaptions)\n",
    "vCaptions = list(vCaptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(vCaptions)\n",
    "train_seqs = tokenizer.texts_to_sequences(vCaptions)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "train_seqs = tokenizer.texts_to_sequences(vCaptions) #train_seqs has the tokenized captions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "135680\n135680\n"
     ]
    }
   ],
   "source": [
    "print(len(train_seqs))\n",
    "print(len(vFeatures)) ##vFeatures and their corressponding tokenised captions in train_seqs\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_cap_vector = collections.defaultdict(list)\n",
    "for vid, cap in zip(vFeatures, cap_vector):\n",
    "  video_to_cap_vector[vid].append(cap)\n",
    "\n",
    "# Create training and validation sets using an 80-20 split randomly.\n",
    "vid_keys = list(video_to_cap_vector.keys())\n",
    "random.seed(0)\n",
    "random.shuffle(vid_keys)\n",
    "\n",
    "slice_index = int(len(vid_keys)*0.7)\n",
    "vid_name_train_keys, vid_name_val_keys = vid_keys[:slice_index], vid_keys[slice_index:]\n",
    "\n",
    "vid_name_train = []\n",
    "cap_train = []\n",
    "for vidt in vid_name_train_keys:\n",
    "  capt_len = len(video_to_cap_vector[vidt])\n",
    "  vid_name_train.extend([vidt] * capt_len)\n",
    "  cap_train.extend(video_to_cap_vector[vidt])\n",
    "\n",
    "vid_name_val = []\n",
    "cap_val = []\n",
    "for vidv in vid_name_val_keys:\n",
    "  capv_len = len(video_to_cap_vector[vidv])\n",
    "  vid_name_val.extend([vidv] * capv_len)\n",
    "  cap_val.extend(video_to_cap_vector[vidv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(vid_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "encoding_dim = 128\n",
    "attention_features_shape = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(vid_name, cap):\n",
    "  vid_tensor = np.load(vid_name.decode('utf-8')+'.npy')\n",
    "  return vid_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((vid_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "    w1 = self.W1(features)\n",
    "    w2 = self.W2(hidden_with_time_axis)\n",
    "    attention_hidden_layer = (tf.nn.tanh(w1 + w2))\n",
    "    score = self.V(attention_hidden_layer)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        self.lstm1 = tf.keras.layers.LSTM(encoding_dim,\n",
    "                                          return_sequences=True\n",
    "                                          )\n",
    "        self.lstm2 = tf.keras.layers.LSTM(encoding_dim,\n",
    "                                          return_sequences=True\n",
    "                                          )\n",
    "        \n",
    "    def call(self, x, training=True):\n",
    "        output1 =  self.lstm1(x,training=training)\n",
    "        output2 = self.lstm2(output1,training=training)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(self.units,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True)\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector,1), x], axis=-1)\n",
    "        output, state, _ = self.lstm(x)\n",
    "        x = self.fc1(output)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        x = self.fc2(x)\n",
    "        return x, state\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = RNN_Encoder(encoding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints_meh/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "  # restoring the latest checkpoint in checkpoint_path\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['_CHECKPOINTABLE_OBJECT_GRAPH',\n",
       " 'decoder/attention/V/bias/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/V/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/V/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/V/kernel/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/V/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/V/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/W1/bias/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/W1/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/W1/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/W1/kernel/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/W1/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/W1/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/W2/bias/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/W2/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/W2/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/W2/kernel/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/W2/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/attention/W2/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/embedding/embeddings/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/embedding/embeddings/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/fc1/bias/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/fc1/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/fc1/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/fc1/kernel/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/fc1/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/fc1/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/fc2/bias/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/fc2/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/fc2/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/fc2/kernel/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/fc2/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/fc2/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/lstm/cell/bias/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/lstm/cell/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/lstm/cell/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/lstm/cell/kernel/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/lstm/cell/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/lstm/cell/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/lstm/cell/recurrent_kernel/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/lstm/cell/recurrent_kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'decoder/lstm/cell/recurrent_kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm1/cell/bias/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm1/cell/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm1/cell/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm1/cell/kernel/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm1/cell/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm1/cell/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm1/cell/recurrent_kernel/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm1/cell/recurrent_kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm1/cell/recurrent_kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm2/cell/bias/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm2/cell/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm2/cell/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm2/cell/kernel/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm2/cell/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm2/cell/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm2/cell/recurrent_kernel/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm2/cell/recurrent_kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'encoder/lstm2/cell/recurrent_kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'optimizer/beta_1/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'optimizer/beta_2/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'optimizer/decay/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'optimizer/learning_rate/.ATTRIBUTES/VARIABLE_VALUE',\n",
       " 'save_counter/.ATTRIBUTES/VARIABLE_VALUE']"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "print(start_epoch)\n",
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(vid_tensor, target):\n",
    "  loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "  out_list = []\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "      out1, out2 = encoder(vid_tensor)\n",
    "      features = tf.concat([out1, out2], 2)\n",
    "\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # passing the features through the decoder\n",
    "          predictions, hidden = decoder(dec_input, features, hidden)\n",
    "\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 Batch 0 Loss 1.2915\n",
      "Epoch 1 Batch 100 Loss 0.7078\n",
      "Epoch 1 Batch 200 Loss 0.7006\n",
      "Epoch 1 Batch 300 Loss 0.6253\n",
      "Epoch 1 Loss 0.708527\n",
      "Time taken for 1 epoch 440.0467493534088 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.6366\n",
      "Epoch 2 Batch 100 Loss 0.5700\n",
      "Epoch 2 Batch 200 Loss 0.5964\n",
      "Epoch 2 Batch 300 Loss 0.5645\n",
      "Epoch 2 Loss 0.581762\n",
      "Time taken for 1 epoch 354.1756474971771 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.5436\n",
      "Epoch 3 Batch 100 Loss 0.5282\n",
      "Epoch 3 Batch 200 Loss 0.5634\n",
      "Epoch 3 Batch 300 Loss 0.5227\n",
      "Epoch 3 Loss 0.535426\n",
      "Time taken for 1 epoch 340.23296761512756 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.5150\n",
      "Epoch 4 Batch 100 Loss 0.5050\n",
      "Epoch 4 Batch 200 Loss 0.5141\n",
      "Epoch 4 Batch 300 Loss 0.4512\n",
      "Epoch 4 Loss 0.507340\n",
      "Time taken for 1 epoch 340.8348412513733 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.5221\n",
      "Epoch 5 Batch 100 Loss 0.4683\n",
      "Epoch 5 Batch 200 Loss 0.4845\n",
      "Epoch 5 Batch 300 Loss 0.4649\n",
      "Epoch 5 Loss 0.486849\n",
      "Time taken for 1 epoch 341.2730703353882 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.5206\n",
      "Epoch 6 Batch 100 Loss 0.4510\n",
      "Epoch 6 Batch 200 Loss 0.4770\n",
      "Epoch 6 Batch 300 Loss 0.4645\n",
      "Epoch 6 Loss 0.469714\n",
      "Time taken for 1 epoch 342.6782476902008 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.4786\n",
      "Epoch 7 Batch 100 Loss 0.4257\n",
      "Epoch 7 Batch 200 Loss 0.4621\n",
      "Epoch 7 Batch 300 Loss 0.4187\n",
      "Epoch 7 Loss 0.454572\n",
      "Time taken for 1 epoch 341.0731225013733 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.4706\n",
      "Epoch 8 Batch 100 Loss 0.4556\n",
      "Epoch 8 Batch 200 Loss 0.4509\n",
      "Epoch 8 Batch 300 Loss 0.4142\n",
      "Epoch 8 Loss 0.440760\n",
      "Time taken for 1 epoch 341.306592464447 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.4528\n",
      "Epoch 9 Batch 100 Loss 0.4239\n",
      "Epoch 9 Batch 200 Loss 0.4519\n",
      "Epoch 9 Batch 300 Loss 0.4089\n",
      "Epoch 9 Loss 0.427839\n",
      "Time taken for 1 epoch 341.2084045410156 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.4234\n",
      "Epoch 10 Batch 100 Loss 0.4163\n",
      "Epoch 10 Batch 200 Loss 0.4217\n",
      "Epoch 10 Batch 300 Loss 0.4166\n",
      "Epoch 10 Loss 0.415684\n",
      "Time taken for 1 epoch 340.71742820739746 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.3974\n",
      "Epoch 11 Batch 100 Loss 0.3910\n",
      "Epoch 11 Batch 200 Loss 0.4407\n",
      "Epoch 11 Batch 300 Loss 0.3950\n",
      "Epoch 11 Loss 0.404123\n",
      "Time taken for 1 epoch 341.2959716320038 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.4077\n",
      "Epoch 12 Batch 100 Loss 0.3818\n",
      "Epoch 12 Batch 200 Loss 0.4099\n",
      "Epoch 12 Batch 300 Loss 0.3578\n",
      "Epoch 12 Loss 0.392975\n",
      "Time taken for 1 epoch 340.9876027107239 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.3633\n",
      "Epoch 13 Batch 100 Loss 0.3738\n",
      "Epoch 13 Batch 200 Loss 0.3830\n",
      "Epoch 13 Batch 300 Loss 0.3666\n",
      "Epoch 13 Loss 0.382743\n",
      "Time taken for 1 epoch 340.9334146976471 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.3612\n",
      "Epoch 14 Batch 100 Loss 0.3747\n",
      "Epoch 14 Batch 200 Loss 0.4076\n",
      "Epoch 14 Batch 300 Loss 0.3478\n",
      "Epoch 14 Loss 0.372480\n",
      "Time taken for 1 epoch 340.5403165817261 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.3855\n",
      "Epoch 15 Batch 100 Loss 0.3608\n",
      "Epoch 15 Batch 200 Loss 0.3669\n",
      "Epoch 15 Batch 300 Loss 0.3480\n",
      "Epoch 15 Loss 0.363056\n",
      "Time taken for 1 epoch 340.789071559906 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.3458\n",
      "Epoch 16 Batch 100 Loss 0.3580\n",
      "Epoch 16 Batch 200 Loss 0.3711\n",
      "Epoch 16 Batch 300 Loss 0.3507\n",
      "Epoch 16 Loss 0.353874\n",
      "Time taken for 1 epoch 340.82626128196716 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.3547\n",
      "Epoch 17 Batch 100 Loss 0.3567\n",
      "Epoch 17 Batch 200 Loss 0.3710\n",
      "Epoch 17 Batch 300 Loss 0.3331\n",
      "Epoch 17 Loss 0.344748\n",
      "Time taken for 1 epoch 340.90239810943604 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.3500\n",
      "Epoch 18 Batch 100 Loss 0.3276\n",
      "Epoch 18 Batch 200 Loss 0.3561\n",
      "Epoch 18 Batch 300 Loss 0.3392\n",
      "Epoch 18 Loss 0.336494\n",
      "Time taken for 1 epoch 340.69795083999634 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.3300\n",
      "Epoch 19 Batch 100 Loss 0.3193\n",
      "Epoch 19 Batch 200 Loss 0.3466\n",
      "Epoch 19 Batch 300 Loss 0.3167\n",
      "Epoch 19 Loss 0.328264\n",
      "Time taken for 1 epoch 341.19984674453735 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.3417\n",
      "Epoch 20 Batch 100 Loss 0.3168\n",
      "Epoch 20 Batch 200 Loss 0.3108\n",
      "Epoch 20 Batch 300 Loss 0.3113\n",
      "Epoch 20 Loss 0.320485\n",
      "Time taken for 1 epoch 341.2489421367645 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.3149\n",
      "Epoch 21 Batch 100 Loss 0.3065\n",
      "Epoch 21 Batch 200 Loss 0.3238\n",
      "Epoch 21 Batch 300 Loss 0.2971\n",
      "Epoch 21 Loss 0.312940\n",
      "Time taken for 1 epoch 341.21616435050964 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.3098\n",
      "Epoch 22 Batch 100 Loss 0.2859\n",
      "Epoch 22 Batch 200 Loss 0.3190\n",
      "Epoch 22 Batch 300 Loss 0.3039\n",
      "Epoch 22 Loss 0.305635\n",
      "Time taken for 1 epoch 340.6739113330841 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.2933\n",
      "Epoch 23 Batch 100 Loss 0.2716\n",
      "Epoch 23 Batch 200 Loss 0.3056\n",
      "Epoch 23 Batch 300 Loss 0.3019\n",
      "Epoch 23 Loss 0.298853\n",
      "Time taken for 1 epoch 341.0245518684387 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.3065\n",
      "Epoch 24 Batch 100 Loss 0.2621\n",
      "Epoch 24 Batch 200 Loss 0.3049\n",
      "Epoch 24 Batch 300 Loss 0.2876\n",
      "Epoch 24 Loss 0.292322\n",
      "Time taken for 1 epoch 340.70754408836365 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.2892\n",
      "Epoch 25 Batch 100 Loss 0.2842\n",
      "Epoch 25 Batch 200 Loss 0.2987\n",
      "Epoch 25 Batch 300 Loss 0.2592\n",
      "Epoch 25 Loss 0.285950\n",
      "Time taken for 1 epoch 340.8654263019562 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.3015\n",
      "Epoch 26 Batch 100 Loss 0.2700\n",
      "Epoch 26 Batch 200 Loss 0.2916\n",
      "Epoch 26 Batch 300 Loss 0.2557\n",
      "Epoch 26 Loss 0.280538\n",
      "Time taken for 1 epoch 341.4447946548462 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.2912\n",
      "Epoch 27 Batch 100 Loss 0.2774\n",
      "Epoch 27 Batch 200 Loss 0.2786\n",
      "Epoch 27 Batch 300 Loss 0.2639\n",
      "Epoch 27 Loss 0.274387\n",
      "Time taken for 1 epoch 340.85095024108887 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.2789\n",
      "Epoch 28 Batch 100 Loss 0.2657\n",
      "Epoch 28 Batch 200 Loss 0.2775\n",
      "Epoch 28 Batch 300 Loss 0.2547\n",
      "Epoch 28 Loss 0.268916\n",
      "Time taken for 1 epoch 341.4271447658539 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.2755\n",
      "Epoch 29 Batch 100 Loss 0.2432\n",
      "Epoch 29 Batch 200 Loss 0.2686\n",
      "Epoch 29 Batch 300 Loss 0.2532\n",
      "Epoch 29 Loss 0.263527\n",
      "Time taken for 1 epoch 341.1548767089844 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.2621\n",
      "Epoch 30 Batch 100 Loss 0.2419\n",
      "Epoch 30 Batch 200 Loss 0.2726\n",
      "Epoch 30 Batch 300 Loss 0.2504\n",
      "Epoch 30 Loss 0.258619\n",
      "Time taken for 1 epoch 340.89940118789673 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.2582\n",
      "Epoch 31 Batch 100 Loss 0.2275\n",
      "Epoch 31 Batch 200 Loss 0.2583\n",
      "Epoch 31 Batch 300 Loss 0.2374\n",
      "Epoch 31 Loss 0.253741\n",
      "Time taken for 1 epoch 341.2082087993622 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.2431\n",
      "Epoch 32 Batch 100 Loss 0.2246\n",
      "Epoch 32 Batch 200 Loss 0.2494\n",
      "Epoch 32 Batch 300 Loss 0.2401\n",
      "Epoch 32 Loss 0.249151\n",
      "Time taken for 1 epoch 341.02097249031067 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.2430\n",
      "Epoch 33 Batch 100 Loss 0.2343\n",
      "Epoch 33 Batch 200 Loss 0.2488\n",
      "Epoch 33 Batch 300 Loss 0.2392\n",
      "Epoch 33 Loss 0.244408\n",
      "Time taken for 1 epoch 340.942179441452 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.2742\n",
      "Epoch 34 Batch 100 Loss 0.2314\n",
      "Epoch 34 Batch 200 Loss 0.2550\n",
      "Epoch 34 Batch 300 Loss 0.2423\n",
      "Epoch 34 Loss 0.240263\n",
      "Time taken for 1 epoch 340.8464539051056 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.2487\n",
      "Epoch 35 Batch 100 Loss 0.2281\n",
      "Epoch 35 Batch 200 Loss 0.2419\n",
      "Epoch 35 Batch 300 Loss 0.2217\n",
      "Epoch 35 Loss 0.236166\n",
      "Time taken for 1 epoch 341.206946849823 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.2383\n",
      "Epoch 36 Batch 100 Loss 0.2282\n",
      "Epoch 36 Batch 200 Loss 0.2409\n",
      "Epoch 36 Batch 300 Loss 0.2272\n",
      "Epoch 36 Loss 0.232420\n",
      "Time taken for 1 epoch 340.74093317985535 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.2252\n",
      "Epoch 37 Batch 100 Loss 0.2251\n",
      "Epoch 37 Batch 200 Loss 0.2178\n",
      "Epoch 37 Batch 300 Loss 0.2022\n",
      "Epoch 37 Loss 0.228927\n",
      "Time taken for 1 epoch 340.96428656578064 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.2203\n",
      "Epoch 38 Batch 100 Loss 0.2130\n",
      "Epoch 38 Batch 200 Loss 0.2397\n",
      "Epoch 38 Batch 300 Loss 0.2204\n",
      "Epoch 38 Loss 0.225242\n",
      "Time taken for 1 epoch 340.9381687641144 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.2273\n",
      "Epoch 39 Batch 100 Loss 0.2131\n",
      "Epoch 39 Batch 200 Loss 0.2298\n",
      "Epoch 39 Batch 300 Loss 0.2180\n",
      "Epoch 39 Loss 0.221641\n",
      "Time taken for 1 epoch 340.7249689102173 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.2200\n",
      "Epoch 40 Batch 100 Loss 0.1991\n",
      "Epoch 40 Batch 200 Loss 0.2305\n",
      "Epoch 40 Batch 300 Loss 0.2346\n",
      "Epoch 40 Loss 0.219373\n",
      "Time taken for 1 epoch 340.7970087528229 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.2207\n",
      "Epoch 41 Batch 100 Loss 0.2109\n",
      "Epoch 41 Batch 200 Loss 0.2224\n",
      "Epoch 41 Batch 300 Loss 0.2164\n",
      "Epoch 41 Loss 0.215172\n",
      "Time taken for 1 epoch 341.2113461494446 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.2341\n",
      "Epoch 42 Batch 100 Loss 0.2028\n",
      "Epoch 42 Batch 200 Loss 0.2284\n",
      "Epoch 42 Batch 300 Loss 0.2051\n",
      "Epoch 42 Loss 0.211650\n",
      "Time taken for 1 epoch 340.83524799346924 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.2145\n",
      "Epoch 43 Batch 100 Loss 0.2087\n",
      "Epoch 43 Batch 200 Loss 0.2074\n",
      "Epoch 43 Batch 300 Loss 0.2070\n",
      "Epoch 43 Loss 0.208617\n",
      "Time taken for 1 epoch 340.84227752685547 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.2139\n",
      "Epoch 44 Batch 100 Loss 0.1960\n",
      "Epoch 44 Batch 200 Loss 0.2148\n",
      "Epoch 44 Batch 300 Loss 0.2040\n",
      "Epoch 44 Loss 0.206418\n",
      "Time taken for 1 epoch 341.32393503189087 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.2008\n",
      "Epoch 45 Batch 100 Loss 0.1919\n",
      "Epoch 45 Batch 200 Loss 0.2061\n",
      "Epoch 45 Batch 300 Loss 0.2008\n",
      "Epoch 45 Loss 0.203324\n",
      "Time taken for 1 epoch 341.3226852416992 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.1967\n",
      "Epoch 46 Batch 100 Loss 0.1909\n",
      "Epoch 46 Batch 200 Loss 0.1945\n",
      "Epoch 46 Batch 300 Loss 0.1926\n",
      "Epoch 46 Loss 0.200613\n",
      "Time taken for 1 epoch 341.12206506729126 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.1924\n",
      "Epoch 47 Batch 100 Loss 0.1869\n",
      "Epoch 47 Batch 200 Loss 0.1975\n",
      "Epoch 47 Batch 300 Loss 0.1898\n",
      "Epoch 47 Loss 0.198387\n",
      "Time taken for 1 epoch 341.1131019592285 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.1953\n",
      "Epoch 48 Batch 100 Loss 0.2015\n",
      "Epoch 48 Batch 200 Loss 0.1790\n",
      "Epoch 48 Batch 300 Loss 0.1879\n",
      "Epoch 48 Loss 0.195874\n",
      "Time taken for 1 epoch 341.5564203262329 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.1923\n",
      "Epoch 49 Batch 100 Loss 0.1982\n",
      "Epoch 49 Batch 200 Loss 0.2079\n",
      "Epoch 49 Batch 300 Loss 0.1896\n",
      "Epoch 49 Loss 0.193211\n",
      "Time taken for 1 epoch 341.13087821006775 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.1996\n",
      "Epoch 50 Batch 100 Loss 0.1698\n",
      "Epoch 50 Batch 200 Loss 0.1861\n",
      "Epoch 50 Batch 300 Loss 0.1860\n",
      "Epoch 50 Loss 0.191266\n",
      "Time taken for 1 epoch 341.22620940208435 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (vid_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(vid_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 Batch 0 Loss 0.1288\n",
      "Epoch 1 Batch 100 Loss 0.1222\n",
      "Epoch 1 Batch 200 Loss 0.1358\n",
      "Epoch 1 Batch 300 Loss 0.1290\n",
      "Epoch 1 Loss 0.128521\n",
      "Time taken for 1 epoch 339.8004479408264 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.1327\n",
      "Epoch 2 Batch 100 Loss 0.1227\n",
      "Epoch 2 Batch 200 Loss 0.1305\n",
      "Epoch 2 Batch 300 Loss 0.1293\n",
      "Epoch 2 Loss 0.127817\n",
      "Time taken for 1 epoch 352.0299656391144 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1282\n",
      "Epoch 3 Batch 100 Loss 0.1214\n",
      "Epoch 3 Batch 200 Loss 0.1337\n",
      "Epoch 3 Batch 300 Loss 0.1253\n",
      "Epoch 3 Loss 0.127828\n",
      "Time taken for 1 epoch 341.46807861328125 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1332\n",
      "Epoch 4 Batch 100 Loss 0.1206\n",
      "Epoch 4 Batch 200 Loss 0.1291\n",
      "Epoch 4 Batch 300 Loss 0.1221\n",
      "Epoch 4 Loss 0.126247\n",
      "Time taken for 1 epoch 338.1523551940918 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1222\n",
      "Epoch 5 Batch 100 Loss 0.1208\n",
      "Epoch 5 Batch 200 Loss 0.1213\n",
      "Epoch 5 Batch 300 Loss 0.1238\n",
      "Epoch 5 Loss 0.125375\n",
      "Time taken for 1 epoch 337.62411975860596 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1218\n",
      "Epoch 6 Batch 100 Loss 0.1211\n",
      "Epoch 6 Batch 200 Loss 0.1291\n",
      "Epoch 6 Batch 300 Loss 0.1282\n",
      "Epoch 6 Loss 0.124870\n",
      "Time taken for 1 epoch 337.46868920326233 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1330\n",
      "Epoch 7 Batch 100 Loss 0.1147\n",
      "Epoch 7 Batch 200 Loss 0.1298\n",
      "Epoch 7 Batch 300 Loss 0.1237\n",
      "Epoch 7 Loss 0.124050\n",
      "Time taken for 1 epoch 337.68471002578735 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.1297\n",
      "Epoch 8 Batch 100 Loss 0.1149\n",
      "Epoch 8 Batch 200 Loss 0.1250\n",
      "Epoch 8 Batch 300 Loss 0.1190\n",
      "Epoch 8 Loss 0.124081\n",
      "Time taken for 1 epoch 337.6621663570404 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.1301\n",
      "Epoch 9 Batch 100 Loss 0.1233\n",
      "Epoch 9 Batch 200 Loss 0.1195\n",
      "Epoch 9 Batch 300 Loss 0.1238\n",
      "Epoch 9 Loss 0.123524\n",
      "Time taken for 1 epoch 337.55229663848877 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.1336\n",
      "Epoch 10 Batch 100 Loss 0.1243\n",
      "Epoch 10 Batch 200 Loss 0.1318\n",
      "Epoch 10 Batch 300 Loss 0.1191\n",
      "Epoch 10 Loss 0.121910\n",
      "Time taken for 1 epoch 337.5065667629242 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.1262\n",
      "Epoch 11 Batch 100 Loss 0.1193\n",
      "Epoch 11 Batch 200 Loss 0.1267\n",
      "Epoch 11 Batch 300 Loss 0.1214\n",
      "Epoch 11 Loss 0.121794\n",
      "Time taken for 1 epoch 337.8092908859253 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.1287\n",
      "Epoch 12 Batch 100 Loss 0.1130\n",
      "Epoch 12 Batch 200 Loss 0.1240\n",
      "Epoch 12 Batch 300 Loss 0.1178\n",
      "Epoch 12 Loss 0.121346\n",
      "Time taken for 1 epoch 338.38366508483887 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.1274\n",
      "Epoch 13 Batch 100 Loss 0.1217\n",
      "Epoch 13 Batch 200 Loss 0.1190\n",
      "Epoch 13 Batch 300 Loss 0.1253\n",
      "Epoch 13 Loss 0.121747\n",
      "Time taken for 1 epoch 338.19173216819763 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.1284\n",
      "Epoch 14 Batch 100 Loss 0.1220\n",
      "Epoch 14 Batch 200 Loss 0.1204\n",
      "Epoch 14 Batch 300 Loss 0.1257\n",
      "Epoch 14 Loss 0.121197\n",
      "Time taken for 1 epoch 338.3096823692322 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.1311\n",
      "Epoch 15 Batch 100 Loss 0.1182\n",
      "Epoch 15 Batch 200 Loss 0.1209\n",
      "Epoch 15 Batch 300 Loss 0.1168\n",
      "Epoch 15 Loss 0.119729\n",
      "Time taken for 1 epoch 338.5425624847412 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.1163\n",
      "Epoch 16 Batch 100 Loss 0.1076\n",
      "Epoch 16 Batch 200 Loss 0.1260\n",
      "Epoch 16 Batch 300 Loss 0.1163\n",
      "Epoch 16 Loss 0.119479\n",
      "Time taken for 1 epoch 338.53883266448975 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.1292\n",
      "Epoch 17 Batch 100 Loss 0.1173\n",
      "Epoch 17 Batch 200 Loss 0.1172\n",
      "Epoch 17 Batch 300 Loss 0.1288\n",
      "Epoch 17 Loss 0.119202\n",
      "Time taken for 1 epoch 338.38986802101135 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.1173\n",
      "Epoch 18 Batch 100 Loss 0.1129\n",
      "Epoch 18 Batch 200 Loss 0.1224\n",
      "Epoch 18 Batch 300 Loss 0.1231\n",
      "Epoch 18 Loss 0.118431\n",
      "Time taken for 1 epoch 338.49144411087036 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.1292\n",
      "Epoch 19 Batch 100 Loss 0.1157\n",
      "Epoch 19 Batch 200 Loss 0.1204\n",
      "Epoch 19 Batch 300 Loss 0.1253\n",
      "Epoch 19 Loss 0.117356\n",
      "Time taken for 1 epoch 338.64611077308655 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.1206\n",
      "Epoch 20 Batch 100 Loss 0.1114\n",
      "Epoch 20 Batch 200 Loss 0.1200\n",
      "Epoch 20 Batch 300 Loss 0.1168\n",
      "Epoch 20 Loss 0.117040\n",
      "Time taken for 1 epoch 338.3044457435608 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.1184\n",
      "Epoch 21 Batch 100 Loss 0.1157\n",
      "Epoch 21 Batch 200 Loss 0.1100\n",
      "Epoch 21 Batch 300 Loss 0.1157\n",
      "Epoch 21 Loss 0.117074\n",
      "Time taken for 1 epoch 338.43510818481445 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.1267\n",
      "Epoch 22 Batch 100 Loss 0.1183\n",
      "Epoch 22 Batch 200 Loss 0.1167\n",
      "Epoch 22 Batch 300 Loss 0.1185\n",
      "Epoch 22 Loss 0.116725\n",
      "Time taken for 1 epoch 338.3246169090271 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.1243\n",
      "Epoch 23 Batch 100 Loss 0.1141\n",
      "Epoch 23 Batch 200 Loss 0.1156\n",
      "Epoch 23 Batch 300 Loss 0.1163\n",
      "Epoch 23 Loss 0.116241\n",
      "Time taken for 1 epoch 338.18822979927063 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.1258\n",
      "Epoch 24 Batch 100 Loss 0.1181\n",
      "Epoch 24 Batch 200 Loss 0.1136\n",
      "Epoch 24 Batch 300 Loss 0.1213\n",
      "Epoch 24 Loss 0.115362\n",
      "Time taken for 1 epoch 338.6397240161896 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.1211\n",
      "Epoch 25 Batch 100 Loss 0.1122\n",
      "Epoch 25 Batch 200 Loss 0.1173\n",
      "Epoch 25 Batch 300 Loss 0.1108\n",
      "Epoch 25 Loss 0.115284\n",
      "Time taken for 1 epoch 338.34258246421814 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.1294\n",
      "Epoch 26 Batch 100 Loss 0.1169\n",
      "Epoch 26 Batch 200 Loss 0.1198\n",
      "Epoch 26 Batch 300 Loss 0.1161\n",
      "Epoch 26 Loss 0.115156\n",
      "Time taken for 1 epoch 338.51618909835815 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.1132\n",
      "Epoch 27 Batch 100 Loss 0.1178\n",
      "Epoch 27 Batch 200 Loss 0.1144\n",
      "Epoch 27 Batch 300 Loss 0.1125\n",
      "Epoch 27 Loss 0.114776\n",
      "Time taken for 1 epoch 338.45682644844055 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.1236\n",
      "Epoch 28 Batch 100 Loss 0.1151\n",
      "Epoch 28 Batch 200 Loss 0.1181\n",
      "Epoch 28 Batch 300 Loss 0.1146\n",
      "Epoch 28 Loss 0.113921\n",
      "Time taken for 1 epoch 338.28729367256165 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.1277\n",
      "Epoch 29 Batch 100 Loss 0.1104\n",
      "Epoch 29 Batch 200 Loss 0.1104\n",
      "Epoch 29 Batch 300 Loss 0.1147\n",
      "Epoch 29 Loss 0.113155\n",
      "Time taken for 1 epoch 338.43266129493713 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.1220\n",
      "Epoch 30 Batch 100 Loss 0.1134\n",
      "Epoch 30 Batch 200 Loss 0.1131\n",
      "Epoch 30 Batch 300 Loss 0.1117\n",
      "Epoch 30 Loss 0.112908\n",
      "Time taken for 1 epoch 338.21574091911316 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.1157\n",
      "Epoch 31 Batch 100 Loss 0.1147\n",
      "Epoch 31 Batch 200 Loss 0.1140\n",
      "Epoch 31 Batch 300 Loss 0.1090\n",
      "Epoch 31 Loss 0.112721\n",
      "Time taken for 1 epoch 338.30789399147034 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.1209\n",
      "Epoch 32 Batch 100 Loss 0.1042\n",
      "Epoch 32 Batch 200 Loss 0.1160\n",
      "Epoch 32 Batch 300 Loss 0.1164\n",
      "Epoch 32 Loss 0.112591\n",
      "Time taken for 1 epoch 338.29082703590393 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.1206\n",
      "Epoch 33 Batch 100 Loss 0.1079\n",
      "Epoch 33 Batch 200 Loss 0.1127\n",
      "Epoch 33 Batch 300 Loss 0.1084\n",
      "Epoch 33 Loss 0.112539\n",
      "Time taken for 1 epoch 337.4943778514862 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.1133\n",
      "Epoch 34 Batch 100 Loss 0.1099\n",
      "Epoch 34 Batch 200 Loss 0.1118\n",
      "Epoch 34 Batch 300 Loss 0.1124\n",
      "Epoch 34 Loss 0.111689\n",
      "Time taken for 1 epoch 337.4365954399109 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.1229\n",
      "Epoch 35 Batch 100 Loss 0.1123\n",
      "Epoch 35 Batch 200 Loss 0.1083\n",
      "Epoch 35 Batch 300 Loss 0.1099\n",
      "Epoch 35 Loss 0.110804\n",
      "Time taken for 1 epoch 337.4373390674591 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.1164\n",
      "Epoch 36 Batch 100 Loss 0.1163\n",
      "Epoch 36 Batch 200 Loss 0.1142\n",
      "Epoch 36 Batch 300 Loss 0.1098\n",
      "Epoch 36 Loss 0.110601\n",
      "Time taken for 1 epoch 337.6168704032898 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.1135\n",
      "Epoch 37 Batch 100 Loss 0.1064\n",
      "Epoch 37 Batch 200 Loss 0.1162\n",
      "Epoch 37 Batch 300 Loss 0.1087\n",
      "Epoch 37 Loss 0.110704\n",
      "Time taken for 1 epoch 337.7511291503906 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.1122\n",
      "Epoch 38 Batch 100 Loss 0.1033\n",
      "Epoch 38 Batch 200 Loss 0.1192\n",
      "Epoch 38 Batch 300 Loss 0.1055\n",
      "Epoch 38 Loss 0.110083\n",
      "Time taken for 1 epoch 337.5458221435547 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.1202\n",
      "Epoch 39 Batch 100 Loss 0.1043\n",
      "Epoch 39 Batch 200 Loss 0.1084\n",
      "Epoch 39 Batch 300 Loss 0.1036\n",
      "Epoch 39 Loss 0.109269\n",
      "Time taken for 1 epoch 337.70316219329834 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.1119\n",
      "Epoch 40 Batch 100 Loss 0.1069\n",
      "Epoch 40 Batch 200 Loss 0.1101\n",
      "Epoch 40 Batch 300 Loss 0.1131\n",
      "Epoch 40 Loss 0.108158\n",
      "Time taken for 1 epoch 337.44191217422485 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.1156\n",
      "Epoch 41 Batch 100 Loss 0.1096\n",
      "Epoch 41 Batch 200 Loss 0.1129\n",
      "Epoch 41 Batch 300 Loss 0.0994\n",
      "Epoch 41 Loss 0.107918\n",
      "Time taken for 1 epoch 337.7251076698303 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.1129\n",
      "Epoch 42 Batch 100 Loss 0.1017\n",
      "Epoch 42 Batch 200 Loss 0.1052\n",
      "Epoch 42 Batch 300 Loss 0.1109\n",
      "Epoch 42 Loss 0.108379\n",
      "Time taken for 1 epoch 337.6255729198456 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.1147\n",
      "Epoch 43 Batch 100 Loss 0.1056\n",
      "Epoch 43 Batch 200 Loss 0.1105\n",
      "Epoch 43 Batch 300 Loss 0.0996\n",
      "Epoch 43 Loss 0.107356\n",
      "Time taken for 1 epoch 337.34244775772095 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.1119\n",
      "Epoch 44 Batch 100 Loss 0.1032\n",
      "Epoch 44 Batch 200 Loss 0.1041\n",
      "Epoch 44 Batch 300 Loss 0.1058\n",
      "Epoch 44 Loss 0.107566\n",
      "Time taken for 1 epoch 339.87785148620605 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.1031\n",
      "Epoch 45 Batch 100 Loss 0.1102\n",
      "Epoch 45 Batch 200 Loss 0.1048\n",
      "Epoch 45 Batch 300 Loss 0.1092\n",
      "Epoch 45 Loss 0.107107\n",
      "Time taken for 1 epoch 337.7357156276703 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.1080\n",
      "Epoch 46 Batch 100 Loss 0.1049\n",
      "Epoch 46 Batch 200 Loss 0.1076\n",
      "Epoch 46 Batch 300 Loss 0.1117\n",
      "Epoch 46 Loss 0.106375\n",
      "Time taken for 1 epoch 337.5818123817444 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.1105\n",
      "Epoch 47 Batch 100 Loss 0.1033\n",
      "Epoch 47 Batch 200 Loss 0.1012\n",
      "Epoch 47 Batch 300 Loss 0.1074\n",
      "Epoch 47 Loss 0.106051\n",
      "Time taken for 1 epoch 337.91863346099854 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.1190\n",
      "Epoch 48 Batch 100 Loss 0.1000\n",
      "Epoch 48 Batch 200 Loss 0.1147\n",
      "Epoch 48 Batch 300 Loss 0.1100\n",
      "Epoch 48 Loss 0.105897\n",
      "Time taken for 1 epoch 337.68290305137634 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.1148\n",
      "Epoch 49 Batch 100 Loss 0.1113\n",
      "Epoch 49 Batch 200 Loss 0.1055\n",
      "Epoch 49 Batch 300 Loss 0.1086\n",
      "Epoch 49 Loss 0.106080\n",
      "Time taken for 1 epoch 337.66332268714905 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.1173\n",
      "Epoch 50 Batch 100 Loss 0.1064\n",
      "Epoch 50 Batch 200 Loss 0.1100\n",
      "Epoch 50 Batch 300 Loss 0.1053\n",
      "Epoch 50 Loss 0.105760\n",
      "Time taken for 1 epoch 337.3747637271881 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.1118\n",
      "Epoch 51 Batch 100 Loss 0.1030\n",
      "Epoch 51 Batch 200 Loss 0.1042\n",
      "Epoch 51 Batch 300 Loss 0.1068\n",
      "Epoch 51 Loss 0.104990\n",
      "Time taken for 1 epoch 337.8385920524597 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.1107\n",
      "Epoch 52 Batch 100 Loss 0.1006\n",
      "Epoch 52 Batch 200 Loss 0.1070\n",
      "Epoch 52 Batch 300 Loss 0.0985\n",
      "Epoch 52 Loss 0.104394\n",
      "Time taken for 1 epoch 337.42125177383423 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.1090\n",
      "Epoch 53 Batch 100 Loss 0.1050\n",
      "Epoch 53 Batch 200 Loss 0.1106\n",
      "Epoch 53 Batch 300 Loss 0.1038\n",
      "Epoch 53 Loss 0.103998\n",
      "Time taken for 1 epoch 337.6414611339569 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.1077\n",
      "Epoch 54 Batch 100 Loss 0.0957\n",
      "Epoch 54 Batch 200 Loss 0.1058\n",
      "Epoch 54 Batch 300 Loss 0.0995\n",
      "Epoch 54 Loss 0.104395\n",
      "Time taken for 1 epoch 337.4142105579376 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.1095\n",
      "Epoch 55 Batch 100 Loss 0.1027\n",
      "Epoch 55 Batch 200 Loss 0.1014\n",
      "Epoch 55 Batch 300 Loss 0.1010\n",
      "Epoch 55 Loss 0.104393\n",
      "Time taken for 1 epoch 338.0384523868561 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.1095\n",
      "Epoch 56 Batch 100 Loss 0.1010\n",
      "Epoch 56 Batch 200 Loss 0.1045\n",
      "Epoch 56 Batch 300 Loss 0.0996\n",
      "Epoch 56 Loss 0.104040\n",
      "Time taken for 1 epoch 337.96788263320923 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.1089\n",
      "Epoch 57 Batch 100 Loss 0.1037\n",
      "Epoch 57 Batch 200 Loss 0.1021\n",
      "Epoch 57 Batch 300 Loss 0.1068\n",
      "Epoch 57 Loss 0.103009\n",
      "Time taken for 1 epoch 339.48249983787537 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.1109\n",
      "Epoch 58 Batch 100 Loss 0.1052\n",
      "Epoch 58 Batch 200 Loss 0.1037\n",
      "Epoch 58 Batch 300 Loss 0.0971\n",
      "Epoch 58 Loss 0.102479\n",
      "Time taken for 1 epoch 340.9787516593933 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.1102\n",
      "Epoch 59 Batch 100 Loss 0.1067\n",
      "Epoch 59 Batch 200 Loss 0.1050\n",
      "Epoch 59 Batch 300 Loss 0.0986\n",
      "Epoch 59 Loss 0.102281\n",
      "Time taken for 1 epoch 341.04672622680664 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.1079\n",
      "Epoch 60 Batch 100 Loss 0.1106\n",
      "Epoch 60 Batch 200 Loss 0.1075\n",
      "Epoch 60 Batch 300 Loss 0.1057\n",
      "Epoch 60 Loss 0.102688\n",
      "Time taken for 1 epoch 341.20294737815857 sec\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.0990\n",
      "Epoch 61 Batch 100 Loss 0.1036\n",
      "Epoch 61 Batch 200 Loss 0.1078\n",
      "Epoch 61 Batch 300 Loss 0.1040\n",
      "Epoch 61 Loss 0.102590\n",
      "Time taken for 1 epoch 340.40322828292847 sec\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.0997\n",
      "Epoch 62 Batch 100 Loss 0.1017\n",
      "Epoch 62 Batch 200 Loss 0.1031\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-55c985d1a477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvid_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (vid_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(vid_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'./checkpoints_meh/train/ckpt-21'"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "ckpt_manager.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 385.78125 277.314375 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \nL 378.58125 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m826035cc35\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#m826035cc35\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"97.044886\" xlink:href=\"#m826035cc35\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(90.682386 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"135.090341\" xlink:href=\"#m826035cc35\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(128.727841 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"173.135795\" xlink:href=\"#m826035cc35\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(166.773295 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"211.18125\" xlink:href=\"#m826035cc35\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(204.81875 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"249.226705\" xlink:href=\"#m826035cc35\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(239.682955 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"287.272159\" xlink:href=\"#m826035cc35\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120 -->\n      <g transform=\"translate(277.728409 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"325.317614\" xlink:href=\"#m826035cc35\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 140 -->\n      <g transform=\"translate(315.773864 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.363068\" xlink:href=\"#m826035cc35\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 160 -->\n      <g transform=\"translate(353.819318 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- Epochs -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(193.265625 268.034687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"306.201172\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m3339d2403b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m3339d2403b\" y=\"230.61829\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.1 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(20.878125 234.417508)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m3339d2403b\" y=\"198.012246\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 201.811465)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m3339d2403b\" y=\"165.406203\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(20.878125 169.205422)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m3339d2403b\" y=\"132.80016\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 136.599378)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m3339d2403b\" y=\"100.194116\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(20.878125 103.993335)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m3339d2403b\" y=\"67.588073\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 71.387292)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m3339d2403b\" y=\"34.98203\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0.7 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(20.878125 38.781248)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- Loss -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     </defs>\n     <g transform=\"translate(14.798438 142.005312)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#pfc12564729)\" d=\"M 58.999432 32.201761 \nL 60.901705 73.534797 \nL 62.803977 88.643132 \nL 64.70625 97.800848 \nL 66.608523 104.482037 \nL 68.510795 110.069284 \nL 70.413068 115.006528 \nL 72.315341 119.509834 \nL 76.119886 127.686087 \nL 79.924432 135.090685 \nL 83.728977 141.773468 \nL 89.435795 150.815751 \nL 95.142614 158.726876 \nL 98.947159 163.569006 \nL 102.751705 167.909536 \nL 104.653977 169.987487 \nL 106.55625 171.751887 \nL 108.458523 173.757676 \nL 112.263068 177.298623 \nL 117.969886 181.986165 \nL 119.872159 183.532488 \nL 123.676705 186.220027 \nL 129.383523 189.781871 \nL 131.285795 190.956044 \nL 133.188068 191.695603 \nL 135.090341 193.06527 \nL 136.992614 194.213599 \nL 138.894886 195.202539 \nL 140.797159 195.919695 \nL 144.601705 197.812282 \nL 148.40625 199.357414 \nL 150.308523 200.225811 \nL 154.113068 201.519595 \nL 157.917614 202.943154 \nL 159.819886 203.479192 \nL 161.722159 204.26693 \nL 167.428977 205.892901 \nL 171.233523 207.134206 \nL 173.135795 207.732718 \nL 176.940341 208.563557 \nL 178.842614 209.200195 \nL 182.647159 210.110356 \nL 192.158523 212.318028 \nL 199.767614 213.852229 \nL 203.572159 214.334881 \nL 205.474432 214.923563 \nL 207.376705 215.373282 \nL 211.18125 215.88563 \nL 216.888068 216.840118 \nL 218.790341 216.995284 \nL 220.692614 217.280605 \nL 222.594886 217.909411 \nL 224.497159 218.137827 \nL 226.399432 218.520249 \nL 230.203977 218.824835 \nL 232.10625 219.31544 \nL 235.910795 219.82925 \nL 239.715341 220.235226 \nL 241.617614 220.62138 \nL 243.519886 220.761358 \nL 245.422159 221.081953 \nL 249.226705 221.318668 \nL 251.128977 221.548207 \nL 253.03125 221.544674 \nL 254.933523 222.060307 \nL 256.835795 222.344559 \nL 264.444886 222.948184 \nL 266.347159 223.474177 \nL 273.95625 223.706784 \nL 275.858523 224.185391 \nL 279.663068 224.357254 \nL 285.369886 225.062195 \nL 287.272159 225.051149 \nL 291.076705 225.322628 \nL 292.978977 225.609207 \nL 296.783523 225.67642 \nL 298.685795 225.800336 \nL 302.490341 226.328909 \nL 308.197159 226.512715 \nL 310.099432 226.529959 \nL 313.903977 227.095608 \nL 319.610795 227.330761 \nL 325.317614 228.036428 \nL 327.219886 227.886147 \nL 329.122159 228.219746 \nL 331.024432 228.151205 \nL 338.633523 228.695533 \nL 340.535795 228.635819 \nL 342.438068 228.740181 \nL 346.242614 229.185642 \nL 348.144886 229.314567 \nL 350.047159 229.185268 \nL 351.949432 229.186036 \nL 353.851705 229.300975 \nL 355.753977 229.637261 \nL 359.558523 229.874489 \nL 361.460795 229.741895 \nL 363.363068 229.773661 \nL 363.363068 229.773661 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 239.758125 \nL 43.78125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 239.758125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_19\">\n    <!-- Loss Plot -->\n    <defs>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n    </defs>\n    <g transform=\"translate(184.805625 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"219.34375\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"251.130859\" xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"311.433594\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"339.216797\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"400.398438\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pfc12564729\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(video, test=False):\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    if not test:\n",
    "        vid_tensor_val = np.load(video+'.npy')\n",
    "    else:\n",
    "        vid_tensor_val = video\n",
    "    vid_tensor_val = tf. convert_to_tensor(vid_tensor_val)\n",
    "    vid_tensor_val = tf.reshape(vid_tensor_val, (1, vid_tensor_val.shape[0], vid_tensor_val.shape[1]))\n",
    "\n",
    "    out1, out2 = encoder(vid_tensor_val, training=False)\n",
    "    features = tf.concat([out1, out2], 2)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden= decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "rid = np.random.randint(0, len(vid_name_val))\n",
    "for video, cap  in zip(vid_name_val[:], cap_val[:]):\n",
    "    real_caption = [tokenizer.index_word[i] for i in cap if i not in [0]]\n",
    "    real_caption = real_caption[1:-1]\n",
    "    #real_caption = ' '.join(real_caption)\n",
    "    result = validate(video)\n",
    "    if (result[-1] == '<end>'):\n",
    "        result = result[:-1]\n",
    "    #prediction_caption = ' '.join(result)\n",
    "    actual.append(real_caption)\n",
    "    predicted.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "rid = np.random.randint(0, len(vid_name_val))\n",
    "for video, cap  in zip(vid_name_val[:], cap_val[:]):\n",
    "    real_caption = [tokenizer.index_word[i] for i in cap if i not in [0]]\n",
    "    real_caption = real_caption[1:-1]\n",
    "    result = validate(video)\n",
    "    if (result[-1] == '<end>'):\n",
    "        result = result[:-1]\n",
    "    actual.append(real_caption)\n",
    "    predicted.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = []\n",
    "targ= []\n",
    "for cap, pred in zip(actual, predicted):\n",
    "    ref.append(' '.join(cap))\n",
    "    targ.append(' '.join(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU: 0.268873\n"
     ]
    }
   ],
   "source": [
    "print('BLEU: %f' % corpus_bleu(ref, targ, weights=(1.0, 0, 0, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True caption:  a guying giving santa a check\nPredicted caption two men discuss how he test a guy near middle weather\nTrue caption:  a man in a commercial talking to santa claus\nPredicted caption a man discusses the united states his frame while giving a new clip from the united states\nTrue caption:  a man is dressed as santa\nPredicted caption 2 people shot of police automobile scenes\nTrue caption:  a man is speaking to santa claus\nPredicted caption man in a suit talks outside of a commercial\nTrue caption:  a man is talking to santa claus\nPredicted caption a person talks outside in front of a small set of building obama\nTrue caption:  a man sings a laser bolt to another man\nPredicted caption a man is talking to another person in front of a city\nTrue caption:  a used car <unk> s <unk>\nPredicted caption a clip of a clip from the background\nTrue caption:  a woman talking about relationships\nPredicted caption a clip taken from a movie\nTrue caption:  an attorney commercial showing a lawyer giving santa a check to fix his sled\nPredicted caption a man talks on a desk\nTrue caption:  commercial for some lawyer\nPredicted caption a clip from a movie or many person in front of him\nTrue caption:  man giving santa a check\nPredicted caption a woman speaking spanish and talking about something\nTrue caption:  man shoots <unk> at santa\nPredicted caption a clip taken from a movie\nTrue caption:  man writes a check for santa\nPredicted caption two characters are doing <unk> a major general laughs\nTrue caption:  santa claus is holding a giant check\nPredicted caption two men speak on a bench\nTrue caption:  santa <unk> holds a big check\nPredicted caption <unk> obama talking about a cross\nTrue caption:  santa gets a large check in a commercial\nPredicted caption man with suit talking about a cross\nTrue caption:  santa is holding a large check\nPredicted caption man in a suit talks outside of a dress\nTrue caption:  scene from a tv show\nPredicted caption images of person wearing dress standing in the woods\nTrue caption:  television commercial advertising the services of a local attorney named <unk> <unk>\nPredicted caption a man speaks in front of a building\nTrue caption:  a woman talking about relationships\nPredicted caption people are driving together on a cross over a red shirt over coat\nTrue caption:  a large crowd has gathered around a place and everyone seems to be in their own world\nPredicted caption many people are walking in the open place\nTrue caption:  a women with black dress asking two young ladies for photos\nPredicted caption group of men play a scene as army man playing a man calling her are fighting\nTrue caption:  there is a man is talking with some women\nPredicted caption many people talks before the main male s shoulder of police\nTrue caption:  there is a man talking with some women\nPredicted caption the <unk> clips played to one woman telling something\nTrue caption:  there is a woman is talking with some people\nPredicted caption <unk> of the great original movie where police is playing in the hotel\nTrue caption:  a man is asking people on the street if he can take their photos\nPredicted caption people are <unk> in the woods\nTrue caption:  a man is wandering around town and asking ladies for photos for his photography\nPredicted caption men and women talks about relationships\nTrue caption:  a man politely asks for ladies photos for his fashion photoshoot\nPredicted caption a woman is talking about her love between two other people\nTrue caption:  few girls walking on the street and speaking to a fat lady then poses for a picture\nPredicted caption people are fight with them\nTrue caption:  two girls speaking to a fat lady and then they pose to the person to take picture\nPredicted caption peoples are clapping then waiting to one another\nTrue caption:  a female photographer kneels on a crowded street outside of brightly lit shops to take a picture of two young women posing together\nPredicted caption footage from a movie where police is <unk> against the\nTrue caption:  in a tv program today s blog is shown today fashion is important it exists in the street too\nPredicted caption some guys are fighting in the woods\nTrue caption:  a man is t as king pictures of two woman on a busy street\nPredicted caption people are having a collection of a scene of the great walk though the house\nTrue caption:  dark big man asking girls if he can take their pictures in public\nPredicted caption several pictures of the great pictures\nTrue caption:  a large group of people mill about on a busy street at night\nPredicted caption series of people plays political comedy\nTrue caption:  the person is asking to take the photos of two woman s for street fashion\nPredicted caption many people are fight with each others perfume\nTrue caption:  asian man taking picture of two ladies in a mall\nPredicted caption male and female actors that is saying something to a man\nTrue caption:  a crowd is there and two women are watching something\nPredicted caption many people are walking in the stage\nTrue caption:  a bunch of people fill the city streets in asia\nPredicted caption people are having a collection of police\nTrue caption:  group of people are walking on the big street\nPredicted caption a women is speaking during the dance\nTrue caption:  a laboratory technician wears blue gown mask and hat and works on dna samples\nPredicted caption a lady wearing a suit look at her green colour head and wearing a gray wall at a blue table\nTrue caption:  a laboratory technician wears blue gown mask and hat and works on dna samples\nPredicted caption a woman wearing pink dress is talking in the wall\nTrue caption:  there is a woman in blue is experimenting in a lab\nPredicted caption woman with purple shirt look at the numbers and then there are students in a device that is going to making something with the help of objects\nTrue caption:  a forensic lab <unk> a dna sample from a cigarette butt\nPredicted caption a woman is standing\nTrue caption:  a woman in protective clothing is using a <unk> to mess with something in a dish\nPredicted caption a woman wearing a suit is <unk> through a large <unk>\nTrue caption:  a woman wears a mask hair cap and blue gown as she separates pink and blue matter in a clear dish with <unk> and a small knife\nPredicted caption a woman wearing pink dress sits at her desk\nTrue caption:  a dna that was found in cigarettes was <unk> to suspect\nPredicted caption woman in white shirt is doing large kitchen and a lady puts it onto colorful\nTrue caption:  the woman in the blue clean room suit appears a sample in a petri dish\nPredicted caption a woman doing some photos of <unk> and making a good class to students\nTrue caption:  a forensic lab worker discusses dna evidence gathered for a case\nPredicted caption a women wearing pink top is using an english speaking to students in a building there is <unk> at a table putting her large green blonde woman for putting a pen and get the project\nTrue caption:  a woman wearing a blue <unk> and gloves\nPredicted caption woman in white top is using an english speaking to hillary clinton in front of a large suit group product it to cut out of her coach\nTrue caption:  a lab technician is putting a sample in a dish while another lady explains why\nPredicted caption a woman wearing coat is writing something in her classroom\nTrue caption:  lab technicians are working on a dna profile for a case\nPredicted caption the hospital <unk> is used a <unk> woman in a <unk>\nTrue caption:  a doctor performing a procedure inside of a lab next to a machine\nPredicted caption a woman is preparing something with the help of objects\nTrue caption:  a woman sitting behind a laptop computer while near a machine\nPredicted caption woman uses a large <unk> in front of her coach\nTrue caption:  forensic science and <unk> a dna sample\nPredicted caption a woman speaking french in general in white dress\nTrue caption:  woman in blue clothes is making something in the hospital\nPredicted caption a lady has a bar and the people are kept on the table\nTrue caption:  woman in blue uniform is working with the other workers in the hospital\nPredicted caption the woman in white shirt is on a young lady\nTrue caption:  various workers in blue uniforms working with different chemicals\nPredicted caption a woman wearing color cloth wearing standing discusing inside\nTrue caption:  science lap 2 nurse try to <unk>\nPredicted caption a man stands up to the woman at the table\nTrue caption:  a computer thing with wires is inside a room\nPredicted caption a woman wearing coat is doing some science project\nTrue caption:  a an announcer in a cartoon is conducting an opening ceremony\nPredicted caption two men in space outfit 5 character who is about to their outfits\nTrue caption:  a anime man is on stage\nPredicted caption a man talks about animated characters from the other avengers 2\nTrue caption:  a cartoon announcing the next event in a fake show\nPredicted caption hands touch hands with purple in colour\nTrue caption:  a cartoon character is crying\nPredicted caption a guy talks about the show showing a outfits talking about something\nTrue caption:  a cartoon doing an introduction\nPredicted caption a man is talking to other cartoon character\nTrue caption:  a cartoon is announcing something\nPredicted caption a man is holding a black jacket\nTrue caption:  a man is presenting an act onstage\nPredicted caption a man is talking to another character speaks\nTrue caption:  a man is talking in a show in an animated video\nPredicted caption this is a person in all black man jumps over to mr\nTrue caption:  a man wearing a uniform is talking on stage\nPredicted caption first square pants\nTrue caption:  a man with pink hair announces today s big event\nPredicted caption hands touch hands <unk> in a large chair\nTrue caption:  a person is announcing an event in this cartoon\nPredicted caption there s a man creature jumps around\nTrue caption:  an animated character talks about a big event to an audience in an anime show\nPredicted caption hands touch hands a gun to help to make the animated character and marvel avengers environment\nTrue caption:  an anime character is making a dramatic pose\nPredicted caption the man <unk> the other avengers 2 mask\nTrue caption:  cartoon character is crying\nPredicted caption a guy talks about animated characters from an animated character and another hooded man\nTrue caption:  two cartoons talk together\nPredicted caption a man discusses what women in a black jacket\nTrue caption:  cartoon character is crying\nPredicted caption a man is holding a gun\nTrue caption:  a cartoon announcing the next event in a fake show\nPredicted caption a man looking at a jacket\nTrue caption:  a man wearing a uniform is talking on stage\nPredicted caption there is some men dressed in blue dress is talking\nTrue caption:  a cartoon is announcing something\nPredicted caption a man is discussing about cartoon characters\nTrue caption:  a anime man is on stage\nPredicted caption an older man talks about the film making the cartoon character\nTrue caption:  german chancellor merkel talking on tv and telling about the option of exiting european union\nPredicted caption anchor giving commentary on the <unk>\nTrue caption:  an old women in red shirt is talking here\nPredicted caption two men and a man are speaking on a game\nTrue caption:  a woman in pink dress talking on a tv channel\nPredicted caption a clip of two extract of the show with two adults\nTrue caption:  a women in pink coat telling news at channel\nPredicted caption a clip of two extract of a tv show called difference between two other people\nTrue caption:  there is a old lady talking from a studio\nPredicted caption two persons are talking about an unknown subject\nTrue caption:  there is a suit man talking from the studio\nPredicted caption two people are talking about a discussion\nTrue caption:  german chancellor angela merkel speaks about politics\nPredicted caption two men and a man are speaking on a show\nTrue caption:  a women in pink long sleeves blouse is talking\nPredicted caption a man and woman speak on a show\nTrue caption:  women in <unk> talking something important\nPredicted caption two men and a male discuss guns and are talking to a man talking to a man and asking questions\nTrue caption:  old women talking about something in the tv channel\nPredicted caption two men and a man s telling a story\nTrue caption:  one old man talking about something in the stage\nPredicted caption two men and a boy are speaking on a computer\nTrue caption:  one old women speaking something about in stage\nPredicted caption former president <unk> is hosting\nTrue caption:  a woman is speaking in the television\nPredicted caption a man and a woman are speaking on the screen\nTrue caption:  an old woman is speaking in the television\nPredicted caption man and a woman talking to a panel\nTrue caption:  a female news anchor speaks to the camera for a news segment\nPredicted caption a person talking about a celebrity\nTrue caption:  a speaker of the news is seriously speaks about\nPredicted caption a man and a woman are talking about a large <unk> that is <unk>\nTrue caption:  an old woman in pink dress speaking sitting\nPredicted caption two people are talking about a woman\nTrue caption:  a woman in pink suit reading news\nPredicted caption a man and a boy are talking about two <unk>\nTrue caption:  an older woman is sitting at a desk and talking\nPredicted caption an anchor up and a man <unk> on a tv show\nTrue caption:  angela merkel is on tv talking about an issue\nPredicted caption a man and a woman are speaking on a show\n"
     ]
    }
   ],
   "source": [
    "for cap, pred in zip(ref[:100], targ[:100]):\n",
    "    print(\"True caption: \",cap)\n",
    "    print(\"Predicted caption\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Real Caption: a person is solving a rubik s cube\nPrediction Caption: a person is showing here a rubik s cube\n"
     ]
    }
   ],
   "source": [
    "rid = np.random.randint(0, len(vid_name_val))\n",
    "video = vid_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]][1:-1])\n",
    "result = validate(video)\n",
    "if (result[-1] == '<end>'):\n",
    "        result = result[:-1]\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', ' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'features/video6625'"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitbaseconda049dde4749274eb8b79afe4fa38bdf79",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}